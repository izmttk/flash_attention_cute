# 使用 CUTLASS CuTe 复现 Flash Attention 2 算子

# Requirements

```
torch
transformers
flash-attn

```

# Quick Start

WIP

# Performance

WIP

# Resources

WIP

# Acknowledgement

1. [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)
2. [NVIDIA/cutlass](https://github.com/NVIDIA/cutlass)